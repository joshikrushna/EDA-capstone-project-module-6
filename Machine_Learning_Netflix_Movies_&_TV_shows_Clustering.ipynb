{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "HhfV-JJviCcP",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "bamQiAODYuh1",
        "OH-pJp9IphqM",
        "PIIx-8_IphqN",
        "BZR9WyysphqO",
        "YJ55k-q6phqO",
        "U2RJ9gkRphqQ",
        "x-EpHcCOp1ci",
        "n3dbpmDWp1ck",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "4_0_7-oCpUZd",
        "bn_IUdTipZyH",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "T0VqWOYE6DLQ",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "qjKvONjwE8ra",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshikrushna/EDA-capstone-project-module-6/blob/main/Machine_Learning_Netflix_Movies_%26_TV_shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unsupervised ML - Netflix Movies and TV Shows Clustering**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Name**    - Krushna Joshi\n",
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**\n",
        "\n",
        "![download (6).png](data:image/png;base64,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\n",
        ")"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The objective of this project is to analyze and cluster a dataset related to Netflix. **The dataset consists of various attributes associated with Netflix shows and movies, such as title, genre, release year, duration, rating, and others. The aim is to explore patterns and similarities among the content available on the platform and group them into meaningful clusters.**\n",
        "\n",
        "\n",
        "To begin with, the dataset will be preprocessed by handling missing values, removing irrelevant columns, and transforming categorical variables into numerical representations. Feature engineering techniques may also be applied to extract useful information from the existing attributes.\n",
        "\n",
        "\n",
        "Next, exploratory data analysis (EDA) techniques will be utilized to gain insights into the dataset. **Visualizations and statistical summaries will be used to understand the distribution of variables, identify any trends, and explore relationships between different features.**\n",
        "\n",
        "\n",
        "Once the dataset has been thoroughly analyzed, clustering algorithms such as k-means, hierarchical clustering, or density-based spatial clustering will be employed. These algorithms will group similar Netflix shows and movies together based on their attributes.** The optimal number of clusters will be determined using techniques like the elbow method or silhouette analysis.**\n",
        "\n",
        "\n",
        "After the clustering process, the results will be evaluated and interpreted. **The clusters will be analyzed to understand the common characteristics and patterns within each group. This analysis will provide valuable information for Netflix in terms of content categorization, recommendation systems, and content acquisition strategies.**\n",
        "\n",
        "\n",
        "Finally, the findings and insights from the clustering analysis will be summarized and presented in a clear and concise manner. Visualizations, charts, and graphs will be used to effectively communicate the outcomes of the project. Recommendations may also be provided based on the identified clusters, suggesting potential improvements or strategies for Netflix to enhance user experience and content offerings.\n",
        "\n",
        "\n",
        "**bold text**\n",
        "**In conclusion, this project aims to analyze a Netflix dataset, perform clustering techniques to group similar shows and movies together, and provide insights and recommendations based on the clustering results. The project will contribute to a better understanding of Netflix's content landscape and aid in decision-making processes for the company.**"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task**"
      ],
      "metadata": {
        "id": "bMrbsJIwO0Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, you are required to do\n",
        "\n",
        "1.  Exploratory Data Analysis.\n",
        "\n",
        "2.  Understanding what type content is available in different countries.\n",
        "\n",
        "3.  Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "\n",
        "4.  Clustering similar content by matching text-based features.\n",
        "\n"
      ],
      "metadata": {
        "id": "vP3WvudLO2qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attribute Information**"
      ],
      "metadata": {
        "id": "MApygeG4PEpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  show_id : Unique ID for every Movie / Tv Show\n",
        "\n",
        "2.  type : Identifier - A Movie or TV Show\n",
        "\n",
        "3.  title : Title of the Movie / Tv Show\n",
        "\n",
        "4.  director : Director of the Movie\n",
        "\n",
        "5.  cast : Actors involved in the movie / show\n",
        "\n",
        "6.  country : Country where the movie / show was produced\n",
        "\n",
        "7.  date_added : Date it was added on Netflix\n",
        "\n",
        "8.  release_year : Actual Releaseyear of the movie / show\n",
        "\n",
        "9.  rating : TV Rating of the movie / show\n",
        "\n",
        "10.  duration : Total Duration - in minutes or number of seasons\n",
        "\n",
        "11.  listed_in : Genere\n",
        "\n",
        "12.  description: The Summary description"
      ],
      "metadata": {
        "id": "rH8XMal5PIW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime as dt\n",
        "\n",
        "from scipy.stats import shapiro                                    # hypothesis testing libraries\n",
        "from scipy.stats import levene\n",
        "import scipy\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "import scipy.stats as stats\n",
        "\n",
        "import re, string, unicodedata                                      # ml preprocess libraries\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from yellowbrick.cluster import KElbowVisualizer                         # ml libraries\n",
        "from sklearn.cluster import KMeans\n",
        "import scipy.cluster.hierarchy as shc\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer             # recomender system libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfa03a6-6be8-48a2-af0f-6268723221f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rMF6A9zeRqJE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "a75d4b01-07b7-4602-aea6-e98ea8de87a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f6d3514c078>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv file location\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Capstone Project 6/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\""
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading csv file\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "KvwYx-agT4tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Checking first five rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking last 5 rows\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "p00fO-1GUPT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'we have total {df.shape[0]} rows and {df.shape[1]} columns.')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info(memory_usage = 'deep')"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "null_value = df.isnull().sum()\n",
        "\n",
        "# Check if there are any missing values\n",
        "if null_value.sum() > 0:\n",
        "    print(\"Missing values found:\")\n",
        "    print(null_value)\n",
        "else:\n",
        "    print(\"No missing values found.\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total null values\n",
        "\n",
        "print(\"Total missing value =\",df.isna().sum().sum())"
      ],
      "metadata": {
        "id": "vaiu1UmO2i9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "fig = px.bar(x=null_value[null_value>0], y=null_value[null_value>0].index)\n",
        "fig.update_layout(\n",
        "    title = \"Missing value count\",\n",
        "    xaxis_title = \"No of missing value\",\n",
        "    yaxis_title = \"column name\",)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "APLeG8hW2nUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_percentage = round((null_value/len(df))*100,2)\n",
        "null_percentage"
      ],
      "metadata": {
        "id": "GHcVcFZY3DZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **director** column has highest NaN values 30.7% data is missing\n",
        "*  **cast** column has 9% NaN values\n",
        "*  **country , date_added , rating** this columns also containing missing values"
      ],
      "metadata": {
        "id": "0K4WL5_ZkAHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(x=null_percentage[null_percentage>0], y=null_percentage[null_percentage>0].index)\n",
        "fig.update_layout(\n",
        "    title = \"Missing value percent\",\n",
        "    xaxis_title = \"% of missing value\",\n",
        "    yaxis_title = \"column name\",)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "a66IVdLG3KqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Null Values\n",
        "\n",
        "df['director'].fillna(value='Unknown',inplace=True)\n",
        "df['cast'].fillna(value='Unknown',inplace=True)\n",
        "df['country'].fillna(value='Unknown',inplace=True)\n",
        "df['rating'].fillna(df['rating'].mode()[0],inplace=True)\n",
        "\n",
        "df.dropna(axis=0, inplace = True)"
      ],
      "metadata": {
        "id": "tyIpE-X23ODI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after removing null values df.shape\n",
        "\n",
        "print(f\"No of Row's = {df.shape[0]}\\nNo of Columns = {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "sGP5-MOa3Wsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contain information about various TV shows and movies available on Netflix, including details like the production country, release year, rating, duration, genre, and a description of each title.\n",
        "\n",
        "1.  This data consists of 12 columns and 7787 rows & after handling null values it contain 7777 rows.\n",
        "2.  In this dataset maximum columns are objective type.\n",
        "3.  There are no duplicated records in the dataset.\n",
        "4.  There are many missing values in director, cast, country, date_added, and rating columns, total 3631 missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  show_id : Unique ID for every Movie / Tv Show\n",
        "\n",
        "2.  type : Identifier - A Movie or TV Show\n",
        "\n",
        "3.  title : Title of the Movie / Tv Show\n",
        "\n",
        "4.  director : Director of the Movie\n",
        "\n",
        "5.  cast : Actors involved in the movie / show\n",
        "\n",
        "6.  country : Country where the movie / show was produced\n",
        "\n",
        "7.  date_added : Date it was added on Netflix\n",
        "\n",
        "8.  release_year : Actual Releaseyear of the movie / show\n",
        "\n",
        "9.  rating : TV Rating of the movie / show\n",
        "\n",
        "10.  duration : Total Duration - in minutes or number of seasons\n",
        "\n",
        "11.  listed_in : Genere\n",
        "\n",
        "12.  description: The Summary description"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "IsfQ4TxLqbX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.apply(lambda col: col.unique()))"
      ],
      "metadata": {
        "id": "BOKaSrOF4Pdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top countries\n",
        "df.country.value_counts()"
      ],
      "metadata": {
        "id": "1jKARC4B4Wv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genre of shows\n",
        "df.listed_in.value_counts()"
      ],
      "metadata": {
        "id": "pKGa1gAT4bHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing the primary country and primary genre to simplify the analysis\n",
        "\n",
        "df['country'] = df['country'].apply(lambda x: x.split(',')[0])\n",
        "df['listed_in'] = df['listed_in'].apply(lambda x: x.split(',')[0])"
      ],
      "metadata": {
        "id": "qLbwHghp4fGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contry in which a movie was produced\n",
        "df.country.value_counts()"
      ],
      "metadata": {
        "id": "KYRkZAxG4m2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# genre of shows\n",
        "df.listed_in.value_counts()"
      ],
      "metadata": {
        "id": "FDHeG9HC4qre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the duration column, and changing the datatype to integer\n",
        "df['duration'] = df['duration'].apply(lambda x: int(x.split()[0]))"
      ],
      "metadata": {
        "id": "P7Tsis1E4xkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of seasons for tv shows\n",
        "df[df['type']=='TV Show'].duration.value_counts()"
      ],
      "metadata": {
        "id": "agSZrril41yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Movie length in minutes\n",
        "df[df['type']=='Movie'].duration.unique()"
      ],
      "metadata": {
        "id": "3NKTY-Bg45KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"date_added\"] = pd.to_datetime(df['date_added'], format='mixed') # Typecasting 'date_added' from string to datetime, using format='mixed' to handle various date formats\n",
        "\n",
        "df['month_added'] = df['date_added'].dt.month                  # Adding new attributes month and year of date added\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "\n",
        "df.date_added.min(),df.date_added.max()                      # first and last date on which a show was added on Netflix"
      ],
      "metadata": {
        "id": "fG5-3u1O49bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('date_added', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "UvsVJyiK5RnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age ratings for shows in the dataset\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x='rating',data=df, palette='muted')"
      ],
      "metadata": {
        "id": "iSEQ9miW5UGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age ratings\n",
        "print(list(df.rating.unique()))"
      ],
      "metadata": {
        "id": "fTew5vVK6G7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the values in the rating column\n",
        "ratings = {'TV-MA':'Adults',\n",
        "              'R':'Adults',\n",
        "              'PG-13':'Teens',\n",
        "              'TV-14':'Young Adults',\n",
        "              'TV-PG':'Older Kids',\n",
        "              'NR':'Adults',\n",
        "              'TV-G':'Kids',\n",
        "              'TV-Y':'Kids',\n",
        "              'TV-Y7':'Older Kids',\n",
        "              'PG':'Older Kids',\n",
        "              'G':'Kids',\n",
        "              'NC-17':'Adults',\n",
        "              'TV-Y7-FV':'Older Kids',\n",
        "              'UR':'Adults'}\n",
        "\n",
        "df['target_ages'] = df['rating'].replace(ratings)\n",
        "print(list(df.target_ages.unique()))"
      ],
      "metadata": {
        "id": "7Q6P7PR_6KSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age ratings for shows in the dataset\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x='target_ages',data=df)"
      ],
      "metadata": {
        "id": "GHd3zyJ_6NNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Around 50% of shows on Netflix are produced for adult audience. Followed by young adults, older kids and kids. Netflix has the least number of shows that are specifically produced for teenagers than other age groups.\n",
        "\n",
        "1.  Check unique values in each columns\n",
        "2.  Choosing the primary country and primary genre to simplify the analysis\n",
        "3.  Change alphanumeric value from duration column, and changing the datatype to integer.\n",
        "3.  Creat new date month & year column\n",
        "4.  Change different ratings to 5 categories"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "E9Q6DH8r6ed9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "okKumJSn6h-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1  **Number of Movies and TV Shows in the dataset**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.pie(df['type'].value_counts(), autopct='%1.1f%%')\n",
        "plt.title('Movies and TV Shows in the dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, we got to know the % of different types of shows available in netflix. 69.1% of the data belongs to movies and 30.9% of the data for TV shows."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart can potentially create a positive business impact by providing valuable information for decision-making. businesses can develop tailored marketing campaigns based on the types of shows most watched by the audience."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 **Top 10 Directors on Netflix**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_2 = df[df['director']!='Unknown'].director.value_counts().reset_index().head(10)\n",
        "plot_2"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "fig = px.bar(plot_2, x=plot_2['count'], y=plot_2.director, color=plot_2['director'], text_auto=True)\n",
        "fig.update_layout(\n",
        "    title = \"Top 15 directors with highest number of Movies and Tv Shows\",\n",
        "    xaxis_title = \"directors\",\n",
        "    yaxis_title = \"Movies Directed\"\n",
        "    )\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jN8gBuHz-e9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 movie directors\n",
        "plt.figure(figsize=(10,5))\n",
        "df[~(df['director']=='Unknown') & (df['type']=='Movie')].director.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 movie directors')"
      ],
      "metadata": {
        "id": "jxRDWWKk7q2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 TV show directors\n",
        "plt.figure(figsize=(10,5))\n",
        "df[~(df['director']=='Unknown') & (df['type']=='TV Show')].director.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 TV show directors')"
      ],
      "metadata": {
        "id": "S2jAUSd58BHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that the most popular director in netflix overall & based on movies and tv shows.\n",
        "\n",
        "Raúl Campos, Jan Suter is most popular director based on movies\n",
        "\n",
        "Alastair Forthergill is most popular director of the tv shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "Jan Suter, Raúl Campos, Marcus Raboy, Jay Karas, Cathy Garcia-Molina, Youssef Chahine are the top 5 directors which highest number of movies available in netflix.\n",
        "Alastair Forthergill, Ken Burns, Shin won ho, Iginio Straffi, Rob Seidenglanz are the top 5 directors which highest number of tv shows available in netflix."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3  **Top 10 Countries with Most Content**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_3 = df[~(df['country']=='Unknown')].country.value_counts().head(10).reset_index()\n",
        "plot_3"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig = px.bar(plot_3, x=plot_3['count'], y=plot_3.country, color=plot_3['country'], text_auto=True)\n",
        "fig.update_layout(\n",
        "    title = \"Top 10 countries with the highest number of shows\",\n",
        "    xaxis_title = \"countries\",\n",
        "    yaxis_title = \"number of shows\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dZOBXWKl8bAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# % share of movies / tv shows by top 3 countries\n",
        "df.country.value_counts().nlargest(3).sum()/len(df)*100"
      ],
      "metadata": {
        "id": "Cig6tnOP-p4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# % share of movies / tv shows by top 10 countries\n",
        "df.country.value_counts().nlargest(10).sum()/len(df)*100"
      ],
      "metadata": {
        "id": "7O2pAtVY-tKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  From above count plot we found that the content belongs to United States alone is 2877 and followed by India is 956.\n",
        "2.  56.6% contentent created in top 3 contries and 78.3% created by top 10 country."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from above insight we got to know:\n",
        "\n",
        "1.   The United States is a leading producer of both types of shows, this makes sense since Netflix is a US company.\n",
        "\n",
        "2.   Most of the content created on netflix by majorly U.S & India."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4  **Content Released Over The Years**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Visualizing the year in which the movie / tv show was released\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['release_year'])\n",
        "plt.title('distribution by released year')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram provides an overview of the distribution of movie release years, this allow comparison of the number of content released in different years."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of release years in the histogram shows a general trend of movies being released on Netflix starting from around 1980. The number of releases gradually increases, with significant growth observed from the year 2000 onwards. The highest peak in the distribution is observed between 2010 and 2020, indicating a high number of Movie/Tv shows releases during that period."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights can help create a positive business impact. By understanding the distribution of release years and identifying trends, businesses can make informed decisions regarding content acquisition, production, and marketing strategies."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5  **Shows added each month over the years**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_5 = df['month_added'].value_counts().reset_index()\n",
        "plot_5"
      ],
      "metadata": {
        "id": "orFLl7DR_SRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Number of shows added on different months\n",
        "fig = px.bar(plot_5, x=plot_5['count'], y=plot_5['month_added'], color=plot_5['month_added'], text_auto=True)\n",
        "fig.update_layout(\n",
        "    title = \"Shows added each month over the years\",\n",
        "    xaxis_title = \"month\",\n",
        "    yaxis_title = \"no Shows added\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most of the shows are uploaded either by year ending or beginning.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1.   October, November, December, and January are months in which many tv shows and movies get uploaded to the platform.\n",
        "2.  It might be due to the winter, as in these months people may stay at home and watch tv shows and movies in their free time."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  **Top 10 Genres on Netflix**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Top 10 genres\n",
        "plt.figure(figsize=(10,5))\n",
        "df.listed_in.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 genres')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Share of top 3 genres\n",
        "df.listed_in.value_counts().nlargest(3).sum()/len(df)*100"
      ],
      "metadata": {
        "id": "Sd0mjFFI_pda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Share of top 10 genres\n",
        "df.listed_in.value_counts().nlargest(10).sum()/len(df)*100"
      ],
      "metadata": {
        "id": "AdgSUaQT_tQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 genre for movies\n",
        "plt.figure(figsize=(10,5))\n",
        "df[df['type']=='Movie'].listed_in.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 genres for movies')"
      ],
      "metadata": {
        "id": "2Tz28tw0_wep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 genre for tv shows\n",
        "plt.figure(figsize=(10,5))\n",
        "df[df['type']=='TV Show'].listed_in.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 genres for TV Shows')"
      ],
      "metadata": {
        "id": "NKFL9_QG_04J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that Drama is in top in Movies and international Tv Show is in top in tv shows.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1.  The dramas is the most popular genre followed by comedies and documentaries.\n",
        "2.  These three genres account for about 41% of all movies and TV shows.\n",
        "3.  This value increases to about 82% for top 10 genres."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_7 = df['year_added'].value_counts().reset_index()\n",
        "plot_7"
      ],
      "metadata": {
        "id": "wwrs159GABYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Number of shows added over the years\n",
        "plt.figure(figsize = (10,5))\n",
        "a = sns.countplot(x=df['year_added'], palette = \"muted\")\n",
        "plt.title('Number of shows added each year')\n",
        "plt.xlabel('')\n",
        "for i in a.patches:\n",
        "    a.annotate(f'{int(i.get_height())}', (i.get_x() + i.get_width() / 2, i.get_height()),\n",
        "                ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_show = df[df[\"type\"] == \"TV Show\"]\n",
        "yearly_tv_show_count = tv_show['year_added'].value_counts().sort_index()\n",
        "\n",
        "movie = df[df[\"type\"] == \"Movie\"]\n",
        "yearly_movie_count = movie['year_added'].value_counts().sort_index()"
      ],
      "metadata": {
        "id": "l9izA0VXAGdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_1 = go.Scatter(x=yearly_tv_show_count.index, y=yearly_tv_show_count.values, name=\"TV Shows\")\n",
        "fig_2 = go.Scatter(x=yearly_movie_count.index, y=yearly_movie_count.values, name=\"Movies\")\n",
        "fig = go.Figure(data=[fig_1, fig_2], layout=go.Layout(title=\"Content added over the years\"))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "aUX86moOAgDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "*  Line graph, is a way to visualize the trend of a single variable over time."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trend in the visualization indicates that between 2008 and 2022, there were relatively fewer TV shows and movies added to Netflix. However, starting from 2016, there was a slight increase in content additions. In 2019, there was a significant peak in the number of movies added, while TV shows experienced a similar trend but with a lesser increase compared to movies."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights indicate a positive impact for Netflix as the demand for both TV shows and movies on the platform has been increasing rapidly over the years. This growth presents an opportunity for Netflix to provide more high-quality content to its users, thereby enhancing user satisfaction and engagement."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8  **No of shows based on rating.**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_8 = df['rating'].value_counts().reset_index()\n",
        "plot_8"
      ],
      "metadata": {
        "id": "_0NO1HAsA2Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig = px.bar(plot_8, x=plot_8['count'], y=plot_8['rating'], color=plot_8['rating'])\n",
        "fig.update_layout(\n",
        "    width=900, height=700,\n",
        "    title = 'No of shows based on ratings',\n",
        "    xaxis_title = 'rating',\n",
        "    yaxis_title = 'No of shows'\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above count plot we can clearly see that the most of the ratings are given by TV-MA followed by TV-14 and the least ratings are given by NC-17.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from above insight we got to know:\n",
        "\n",
        "*  TV-MA tops the charts, indicating that mature content is more popular on Netflix.\n",
        "*  This popularity is followed by TV-14 and TV-PG, which are Shows focused on Teens and Older kids.\n",
        "*  Very few titles with a rating NC-17 exist. It can be understood since this type of content is purely for the audience above 1"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Number of movies and TV shows added over the years\n",
        "plt.figure(figsize=(10,5))\n",
        "a = sns.countplot(x='year_added',data=df, hue='type')\n",
        "plt.title('Number of movies and TV shows added over the years')\n",
        "plt.xlabel('')\n",
        "for i in a.patches:\n",
        "    a.annotate(f'{int(i.get_height())}', (i.get_x() + i.get_width() / 2, i.get_height()),\n",
        "                ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of shows released each year since 2008\n",
        "order = range(2008,2022)\n",
        "plt.figure(figsize=(10,5))\n",
        "a = sns.countplot(x='release_year',data=df, hue='type',\n",
        "                  order = order)\n",
        "plt.title('Number of shows released each year since 2008 that are on Netflix')\n",
        "plt.xlabel('')\n",
        "for i in a.patches:\n",
        "    a.annotate(f'{int(i.get_height())}', (i.get_x() + i.get_width() / 2, i.get_height()),\n",
        "                ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "UwZ7ARqLBQUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Bar graph highlights that 2017 and 2020 demonstrate the highest trends. These years exhibit a significant number of content release on Netflix.\n",
        "*   Another bar graph show Number of shows released each year since 2008"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Over the years, Netflix has consistently focused on adding more shows in its platform.\n",
        "*  Though there was a decrease in the number of movies added in 2020, this pattern did not exist in the number of TV shows added in the same year.\n",
        "*  This might signal that Netflix is increasingly concentrating on introducing more TV series to its platform rather than movies."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10  **No of season distributions**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Seasons in each TV show\n",
        "plt.figure(figsize=(10,5))\n",
        "a = sns.countplot(x='duration',data=df[df['type']=='TV Show'], palette=\"muted\")\n",
        "plt.title('Number of seasons per TV show distribution')\n",
        "for i in a.patches:\n",
        "    a.annotate(f'{int(i.get_height())}', (i.get_x() + i.get_width() / 2, i.get_height()),\n",
        "                ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# % of tv shows with just 1 season\n",
        "len(df[(df['type']=='TV Show') & (df['duration']==1)])/len(df[df['type']=='TV Show'])*100"
      ],
      "metadata": {
        "id": "XabvzwO4BuSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart in question is a countplot, which is a type of bar chart that shows the frequency or count of each category in a categorical variable. It seems to be used to display the distribution of TV show seasons"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, we observed that the majority of TV shows or web series in the dataset have only one season, while the remaining shows have a maximum of two, three, four, or five seasons."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes because by recognizing that the majority of TV shows have a limited number of seasons, content producers and streaming platforms can optimize their production planning. They can allocate resources more efficiently, reduce production costs, and potentially increase the output of content."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11  **Duration Distribution for Netflix Movies**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# length of movie analysis\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(x='duration',data=df[df['type']=='Movie'])\n",
        "plt.title('Movie duration distribution')"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Movie statistics\n",
        "df[df['type']== 'Movie'].duration.describe()"
      ],
      "metadata": {
        "id": "9iUUGZz4CH8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that most of the movies last for 90 to 120 minutes.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "*  On netflix most of the movies last for 90 to 120 minutes.\n",
        "*  So for target audience, movies duration will be greater than minimum 90 minutes"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12  **Average movie length over the years**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Average movie length over the years\n",
        "plt.figure(figsize=(10,5))\n",
        "df[df['type']=='Movie'].groupby('release_year')['duration'].mean().plot(kind='line')\n",
        "plt.title('Average movie length over the years')\n",
        "plt.ylabel('Length of movie in minutes')\n",
        "plt.xlabel('Year')"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['type']== 'Movie'].release_year.describe()"
      ],
      "metadata": {
        "id": "XWIyVNkRChCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line graph, is a way to visualize the trend of a single variable over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  On average, movies made in the 1960s have the longest movie length.\n",
        "*  The average length of a movie has been continuously decreasing since the 2000s.\n",
        "*  Netflix has several movies on its site, including those that were released in way back 1942."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By this chart we can see what is the duration of movie consumers like to watch over the years and based on this information we can decide future movie length."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing data for heatmap\n",
        "df['count'] = 1\n",
        "\n",
        "# Group by country and sum counts\n",
        "country_counts = df.groupby('country')['count'].sum().sort_values(ascending=False).reset_index()\n",
        "\n",
        "# Select the top 10 countries\n",
        "top_10_countries = country_counts.head(10)['country']\n",
        "\n",
        "# Filter the original DataFrame to include only the top 10 countries\n",
        "df_heatmap = df[df['country'].isin(top_10_countries)]\n",
        "\n",
        "# Create crosstab for the heatmap\n",
        "df_heatmap = pd.crosstab(df_heatmap['country'], df_heatmap['target_ages'], normalize='index').T\n",
        "print(df_heatmap)\n"
      ],
      "metadata": {
        "id": "gejg7KOQC-Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "country_order2 = ['United States', 'India', 'United Kingdom', 'Canada', 'Japan', 'France', 'South Korea', 'Spain', 'Mexico']\n",
        "age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "\n",
        "sns.heatmap(data=df_heatmap.loc[age_order, country_order2],\n",
        "            cmap='YlGnBu',\n",
        "            square=True,\n",
        "            linewidth=2.5,\n",
        "            cbar=False,\n",
        "            annot=True,\n",
        "            fmt='1.0%',\n",
        "            vmax=.6,\n",
        "            vmin=0.05,\n",
        "            ax=ax,\n",
        "            annot_kws={\"fontsize\": 12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['count'] = 1\n",
        "\n",
        "# Group by listed_in and sum counts\n",
        "genre_counts = df.groupby('listed_in')['count'].sum().sort_values(ascending=False).reset_index()\n",
        "\n",
        "# Select the top 10 genres\n",
        "data1 = genre_counts.head(10)['listed_in']\n",
        "\n",
        "# Display the top 10 genres\n",
        "print(data1)"
      ],
      "metadata": {
        "id": "spAu-mWJHRAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_heatmap1 = df[df['listed_in'].isin(data1)]\n",
        "df_heatmap1 = pd.crosstab(df_heatmap1['listed_in'],df_heatmap1['target_ages'],normalize = \"index\").T\n",
        "df_heatmap1"
      ],
      "metadata": {
        "id": "5xiXPVrFHlU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "top=['Dramas', 'Comedies', 'Documentaries',\n",
        "       'Action & Adventure',\n",
        "       'International TV Shows', \"Children & Family Movies\",\n",
        "       'Crime TV Shows', \"Kids' TV\",\n",
        "       'Stand-Up Comedy',\n",
        "       'Horror Movies']\n",
        "age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "\n",
        "sns.heatmap(data=df_heatmap1.loc[age_order, top],\n",
        "            cmap='YlGnBu',\n",
        "            square=True,\n",
        "            linewidth=2.5,\n",
        "            cbar=False,\n",
        "            annot=True,\n",
        "            fmt='1.0%',\n",
        "            vmax=.6,\n",
        "            vmin=0.05,\n",
        "            ax=ax,\n",
        "            annot_kws={\"fontsize\": 12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6hsJfy5HvFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is a suitable choice when visualizing the relationships between two categorical variables, in this case, (countries and age groups) & (Dramas and age range). It allows for a clear representation of patterns, trends, and comparisons across different categories."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  United States, United Kingdom, Canada, France, Spain, Mexico have maximum no of movies for adult age group\n",
        "2.  We can also see that different category of movie targate different age range."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(df, corner = True,aspect = 1.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot is a suitable choice when visualizing the relationships between different categorical variables. It allows for a clear representation of patterns, trends, and comparisons across different categories."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Based on the plot of release_year and year_added, we can conclude that Netflix is increasingly adding and releasing movies and TV shows over time.\n",
        "2.  We can conclude from plot release_year and month_added that Netflix releases movies and TV shows throughout the all months of the year."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on above chart experiments i have noticed that some variable of our netflix dataset does not seems to normally distributed so i have made hypothetical assumption that our data is normally distributed and for that i have decided to do statistical analysis.\n",
        "\n",
        "1.  Average number of movies on Netflix in United States is greater than the average number of movies on Netflix in India.\n",
        "2.  The number of movies available on Netflix is greater than the number of TV shows available on Netflix."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average number of movies on Netflix in United States is greater than the average number of movies on Netflix in India.\n",
        "\n"
      ],
      "metadata": {
        "id": "2K1rdAKgIULd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: H0 = Average number of movies on Netflix in United States <= average number of movies on Netflix in India.\n",
        "\n",
        "Alternate hypothesis: H1 = Average number of movies on Netflix in United States > average number of movies on Netflix in India.\n",
        "\n",
        "Test Type: Two-sample t-test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_movie = df[(df['type']=='Movie') & (df.country == 'United States')]\n",
        "india_movie = df[(df['type']=='Movie') & (df.country == 'India')]"
      ],
      "metadata": {
        "id": "HU9FOM9eIaOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro\n",
        "\n",
        "# Perform the Shapiro-Wilk test for both us_movie and india_movie\n",
        "shapiro_us_movie = shapiro(us_movie['year_added'])\n",
        "shapiro_india_movie = shapiro(india_movie['year_added'])\n",
        "\n",
        "print(\"Shapiro-Wilk test for us_movie:\", shapiro_us_movie)\n",
        "print(\"Shapiro-Wilk test for india_movie:\", shapiro_india_movie)"
      ],
      "metadata": {
        "id": "-4jeiqzYIc3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In shapiro test pvalue is greater then 0.05 so data is normally distributed so we can apply Independent 2 sample t-test**"
      ],
      "metadata": {
        "id": "ZJQgy9EhIgN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import levene\n",
        "\n",
        "# Perform Levene's test\n",
        "levene_test = levene(us_movie['release_year'], india_movie['release_year'])\n",
        "print(levene_test)"
      ],
      "metadata": {
        "id": "tdIsD7y5IpSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**p-value is 0.344, which is greater than 0.05. Therefore, we can conclude that there is no significant difference in the variances of the release years between the US and India movies.**"
      ],
      "metadata": {
        "id": "VnCGiVkxIrlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the null hypothesis\n",
        "H0 = \"Average number of movies on Netflix in United States <= average number of movies on Netflix in India.\"\n",
        "\n",
        "# Define the alternative hypothesis\n",
        "H1 = \"Average number of movies on Netflix in United States > average number of movies on Netflix in India.\""
      ],
      "metadata": {
        "id": "Zut2W7c5IvOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Perform the two-sample t-test between the release years of the two groups of movies\n",
        "import scipy\n",
        "t_stat, p_val = scipy.stats.ttest_ind(us_movie['release_year'], india_movie['release_year'], equal_var=False)\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_val)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in average number of movies produced by the 'United States' and 'India'.\")\n",
        "else:\n",
        "  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in average number of movies produced by the 'United States' and 'India'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xhYSji8pI3t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the number of movies available on Netflix in the United States and India, I conducted a two-sample t-test, also known as an independent samples t-test or unpaired t-test. I utilized the ttest_ind function from the scipy.stats module to carry out the test, which is suitable for analyzing the means of two independent samples. By applying this test, I was able to calculate the p-value and determine if there is a significant difference in the number of movies between the two countries."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the two-sample t-test for this analysis as it is suitable for comparing the means of two independent samples. In this case, we have two separate sets of movies data from Netflix for the United States and India, and we aim to determine if there is a significant difference in the average number of movies between these two countries."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of **movies** available on Netflix is greater than the number of **TV shows** available on Netflix.\n",
        "\n"
      ],
      "metadata": {
        "id": "bMvKsNIAJErE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: H0 = The number of movies available on Netflix = the number of TV shows available on Netflix\n",
        "\n",
        "Alternate hypothesis: H1 = The number of movies available on Netflix != the number of TV shows available on Netflix\n",
        "\n",
        "Test Type: Two sample z-test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of movies and TV shows in the DataFrame\n",
        "n_movies = df[df['type'] == 'Movie'].count()['type']\n",
        "n_tv_shows = df[df['type'] == 'TV Show'].count()['type']"
      ],
      "metadata": {
        "id": "iF7-xycNJMQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the null hypothesis\n",
        "H0 = \"The number of movies available on Netflix = the number of TV shows available on Netflix\"\n",
        "\n",
        "# Define the alternative hypothesis\n",
        "H1 = \"The number of movies available on Netflix != the number of TV shows available on Netflix\""
      ],
      "metadata": {
        "id": "_E1YWciRJRPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Set the counts and sample sizes for the z-test\n",
        "counts = [n_movies, n_tv_shows]  # Number of movies and TV shows\n",
        "nobs = [len(df), len(df)]  # Total number of observations in the DataFrame\n",
        "\n",
        "# Perform a two sample z-test assuming equal proportions\n",
        "z_stat, p_val = proportions_ztest(counts, nobs, value=0, alternative='larger')\n",
        "\n",
        "# Print the results\n",
        "print('Z-statistic:', z_stat)\n",
        "print('P-value:', p_val)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in number of 'movies' and 'TV shows' available on Netflix.\")\n",
        "else:\n",
        "  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in number of 'movies' and 'TV shows' available on Netflix.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2L3LsitWJWws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the number of movies and TV shows available on Netflix, I conducted a two-sample z-test for proportions to obtain the p-value.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose the two-sample z-test for proportions to compare the number of movies and TV shows available on Netflix because the data consists of two categorical variables."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Duration of movies on Netflix is 99.30 min.\n",
        "\n"
      ],
      "metadata": {
        "id": "LUQpTX4LJfcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: H0 = The average duration of Movie is 99.30 min.\n",
        "\n",
        "Alternate hypothesis: H1 = The average duration of Movie is not 99.30 min.\n",
        "\n",
        "Test Type: One-sample t-test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_duration = df[df['type']== 'Movie'].duration\n",
        "\n",
        "print('mean:',movie_duration.mean())\n",
        "print('standard daviation:',movie_duration.std())\n",
        "print('variance:',movie_duration.var())"
      ],
      "metadata": {
        "id": "No9AeK8eJmPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_movie_duration = movie_duration.sample(45)\n",
        "\n",
        "\n",
        "print('mean:',sample_movie_duration.mean())\n",
        "print('standard daviation:',sample_movie_duration.std())\n",
        "print('variance:',sample_movie_duration.var())"
      ],
      "metadata": {
        "id": "-t78bDeZJohb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Shapiro-Wilk test for sample_movie_duration\n",
        "shapiro_sample_movie_duration = shapiro(sample_movie_duration)\n",
        "print(\"Shapiro-Wilk test for desktop users:\", shapiro_sample_movie_duration)"
      ],
      "metadata": {
        "id": "tmI1cmu_JrGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**in shapiro test pvalue is greater then 0.05 so data is normally distributed so we can apply Single Sample t-test**"
      ],
      "metadata": {
        "id": "Hv5PGxVOJtfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the null hypothesis\n",
        "H0 = (\"The average duration of Movie is 99.30\")\n",
        "\n",
        "# Define the alternative hypothesis\n",
        "H1 = \"The average duration of Movie is not 99.30\""
      ],
      "metadata": {
        "id": "6GdWvM_dJwn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "# Calculate the test statistic\n",
        "t_stat, p_value = stats.ttest_1samp(sample_movie_duration, 99.30)\n",
        "\n",
        "# Print the results\n",
        "print(\"Test statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value/2)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conclusion\n",
        "if p_value/2 < 0.05:\n",
        "    print(f\"Reject the null hypothesis.{H1}\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.{H0}\")"
      ],
      "metadata": {
        "id": "QTQgjLT2J1ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  sample_movie_duration = movie_duration.sample(45)\n",
        "  t_stat, p_value = stats.ttest_1samp(sample_movie_duration, 99.30)\n",
        "  print(\"\\np-value:\", p_value/2)\n",
        "  if p_value/2 < 0.05:\n",
        "    print(f\"Reject the null hypothesis.{H1}\")\n",
        "  else:\n",
        "    print(f\"Fail to reject the null hypothesis.{H0}\")"
      ],
      "metadata": {
        "id": "LoRCZsGwJ4XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the movies duration on Netflix, I conducted a One-sample t-test. I utilized the ttest_1samp function from the scipy.stats module to carry out the test, which is suitable for analyzing the means of one independent samples and one value. By applying this test, I was able to calculate the p-value and determine if there is a significant difference in the duration of movies different random samples."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the one-sample t-test for this analysis as it is suitable for comparing the means of one independent samples corrosponding to a value. In this case, we take random samples of movies duration data from Netflix, and we aim to determine if there is a significant difference in the average Movie duration in different random samples."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()"
      ],
      "metadata": {
        "id": "8AuoFNVqKKpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df1.isna().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its already handled in data wrangling, so now there are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Most of the columns are categorical, so no outliers observed)"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(No need as the data is categorical)\n",
        "\n"
      ],
      "metadata": {
        "id": "vFRg4OFHKbUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Create a new column called 'clustering' in the DataFrame 'df1'\n",
        "# This column is to store text data of different column\n",
        "\n",
        "df1['clustering'] = (df1['director'] + ' ' +\n",
        "                                df1['cast'] +' ' +\n",
        "                                df1['country'] +' ' +\n",
        "                                df1['listed_in'] +' ' +\n",
        "                                df1['description'])"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "Pv6RXr54Kl3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to remove non-ascii characters\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Function to remove non-ASCII characters\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "7uAcGbPtKpWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove non-ascii characters\n",
        "\n",
        "df1['clustering'] = remove_non_ascii(df1['clustering'])\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "TFsYFt1nKrtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Define a function to convert text into lower cases\n",
        "def to_lower(x):\n",
        "  return x.lower()\n",
        "\n",
        "# Apply the to_lower() function to the 'tags' column of the DataFrame\n",
        "df1['clustering'] = df1['clustering'].apply(to_lower)\n",
        "\n",
        "# Cross checking our result for the function created\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "# function to remove punctuations\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation marks\n",
        "df1['clustering'] = df1['clustering'].apply(remove_punctuation)\n",
        "\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "b3VLCYKXK1TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "# 'clustering' column does not have any URLs so remove words and digits containing digits\n",
        "df1['clustering'] = df1['clustering'].str.replace(r'\\w*\\d\\w*', '', regex=True)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "sw = stopwords.words('english')\n",
        "np.array(sw)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove stop words\n",
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word for word in text.split() if word not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "BXPCbkirK_e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words\n",
        "df1['clustering'] = df1['clustering'].apply(stopwords)\n",
        "\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "XzdMgue_LDbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "df1['clustering'] = df1['clustering'].str.strip()\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "def rephrase_tags(x):\n",
        "  return x.replace('mall', 'shoping center')\n",
        "df1['clustering'] = df1['clustering'].apply(rephrase_tags)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "6vJj-PPzLPGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to lemmatize the corpus\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "T67V_RZALSH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('wordnet')\n",
        "\n",
        "# Lemmatization\n",
        "df1['clustering'] = lemmatize_verbs(df1['clustering'])\n",
        "\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "D8h25EiCLUWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Apply the tokenization to the 'tags' column of the DataFrame\n",
        "df1['clustering'] = df1['clustering'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Cross-check our result that the function worked as expected\n",
        "print(df1['clustering'][40])\n",
        "\n",
        "# Store this list form of 'tags' column as 'temp_tags' for later POS tagging purpose\n",
        "temp_tags = df1['clustering']"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_tags[40])"
      ],
      "metadata": {
        "id": "cqpcuqc7LrNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Define a function to Normalize Text function\n",
        "def stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Apply the stemming function to the 'tags' column of the DataFrame\n",
        "df1['clustering'] = df1['clustering'].apply(stemming)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "df1['clustering'][0]"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i use Stemming.\n",
        "\n",
        "Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\". Stemming is important in natural language understanding (NLU) and natural language processing (NLP). Nil means the suffix is replaced with nothing and is just removed. There may be cases where these rules vary depending on the words. As in the case of the suffix 'ed' if the words are 'cared' and 'bumped' they will be stemmed as 'care' and 'bump'.\n",
        "\n",
        "SnowballStemmer:\n",
        "\n",
        "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it. It was originally designed and built by Martin Porter. SnowballStemmer() is a module in NLTK that implements the Snowball stemming technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "# Loading Libraries\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Apply the pos tagging to the 'tags' column of the DataFrame\n",
        "df1['clustering'] = temp_tags.apply(nltk.pos_tag)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "df1['clustering'][40]"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using this tagset we know that which tag shows which type of POS\n",
        "# import nltk\n",
        "# nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "MaPd8orZL6uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function which gives true word (appropriate word) after pos tagging\n",
        "def sentence(data):\n",
        "  x=\"\"\n",
        "  for i in data:\n",
        "    a=i[0]+' '\n",
        "    x=x+a\n",
        "  return x\n",
        "\n",
        "# Apply the sentence function to the 'tags' column of the DataFrame\n",
        "true_word = df1['clustering'].apply(sentence)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(true_word[40])"
      ],
      "metadata": {
        "id": "GkiDN-QWMAb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_word.shape"
      ],
      "metadata": {
        "id": "o8g6nRihMCdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "# setting max features = 20000 to prevent system from crashing\n",
        "\n",
        "# Create the object of tfid vectorizer\n",
        "tfidf = TfidfVectorizer(max_features = 20000)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tfidf.fit_transform(true_word).toarray()\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "805B2tEEMIZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "4l-cJuTeMKmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "id": "dunWdQ0GMNYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_data=pd.DataFrame(X)\n",
        "vec_data"
      ],
      "metadata": {
        "id": "Fv7ETzj7MPpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have use TF-IDF techique for vectorization.\n",
        "\n",
        "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents (also known as a corpus).\n",
        "\n",
        "I have use TF-IDF because TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. I can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Not required)"
      ],
      "metadata": {
        "id": "S3AgBA-LMixO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(No need to transform this data because this data is in form of Text Vectorization)"
      ],
      "metadata": {
        "id": "ETRLGhB2MqnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Here the units of whole data are same so no need to do scaling)"
      ],
      "metadata": {
        "id": "s-eeprUfMuM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes its needed, because dimensionality reduction removes the least important variables from the model. That will reduce the model's complexity and also remove some noise in the data. Its also helps to mitigate overfitting."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using PCA to reduce dimensionality, this takes time.\n",
        "pca = PCA()\n",
        "pca.fit(X)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained variance for different number of components\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title('PCA - Cumulative explained variance vs number of components')\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.axhline(y= 0.8, color='red', linestyle='--')\n",
        "plt.axvline(x= 3400, color='green', linestyle='--')\n",
        "\n",
        "# Display chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OBrj4cmKNI5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reducing the dimensions to 4000 using pca\n",
        "pca = PCA(n_components=3400,random_state=42)\n",
        "pca.fit(X)"
      ],
      "metadata": {
        "id": "PWIVgZ36NLcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformed features\n",
        "x_pca = pca.transform(X)"
      ],
      "metadata": {
        "id": "_bNO6oNBNQlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of transformed vectors\n",
        "x_pca.shape"
      ],
      "metadata": {
        "id": "ziwSWHYDNSk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use PCA to reduce the dimensionality of data.\n",
        "\n",
        "Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines. Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.\n",
        "\n",
        "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Not required)\n",
        "\n"
      ],
      "metadata": {
        "id": "kkiWy3MNNaUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Not required)\n",
        "\n"
      ],
      "metadata": {
        "id": "1rhtQgxpNdH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1  K-Means Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "# from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Finding optimal number of clusters using the elbow method\n",
        "\n",
        "# Instantiate the clustering model and visualizer. this takes time.\n",
        "model = KMeans()\n",
        "visualizer = KElbowVisualizer(model, k=(4, 22), metric='silhouette', timings=False, locate_elbow=True)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x_pca)\n",
        "\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the K-Means clustering model where number of clusters is 6\n",
        "kmean=KMeans(n_clusters=6)\n",
        "\n",
        "# Fit the data to the KMean cluster\n",
        "kmean.fit(x_pca)\n",
        "\n",
        "# Predict on the model\n",
        "y_kmean=kmean.predict(x_pca)"
      ],
      "metadata": {
        "id": "NFDw_9MbSJ6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column 'K_mean_cluster' in the dataset\n",
        "df[\"K_mean_cluster\"]=y_kmean\n",
        "df.head()"
      ],
      "metadata": {
        "id": "TaPYnoy_SNn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting unique labels\n",
        "u_labels = np.unique(y_kmean)\n",
        "\n",
        "# Plotting the results:\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in u_labels:\n",
        "    plt.scatter(x_pca[y_kmean == i , 0] ,x_pca[y_kmean == i , 1] , label = i)\n",
        "plt.title('Clusters for K-Means Clustering')\n",
        "plt.legend()\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_6HLE7P0SQuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Second chart (Scatter plot Graph) shows the result of our algorithm. We can see that cluster wise data distribution in this chart\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I used K-Means Clustering.\n",
        "\n",
        "K means number of clusters.\n",
        "\n",
        "K-means is a centroid-based clustering algorithm, where we calculate the distance between each data point and a centroid to assign it to a cluster. The goal is to identify the K number of groups in the dataset.\n",
        "\n",
        "K-means clustering distinguishes itself from Hierarchical since it creates K random centroids scattered throughout the data. The algorithm looks a little bit like…\n",
        "\n",
        "(1) Initialize K random centroids.\n",
        "\n",
        "You could pick K random data points and make those your starting points.\n",
        "\n",
        "Otherwise, you pick K random values for each variable.\n",
        "\n",
        "(2) For every data point, look at which centroid is nearest to it.\n",
        "\n",
        "Using some sort of measurement like Euclidean or Cosine distance.\n",
        "\n",
        "(3) Assign the data point to the nearest centroid.\n",
        "\n",
        "(4) For every centroid, move the centroid to the average of the points assigned to that centroid.\n",
        "\n",
        "(5) Repeat the last three steps until the centroid assignment no longer changes.\n",
        "\n",
        "The algorithm is said to have “converged” once there are no more changes.\n",
        "\n",
        "These centroids act as the average representation of the points that are assigned to it. This gives you a story almost right away. You can compare the centroid values and tell if one cluster favors a group of variables or if the clusters have logical groupings of key variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ThnA0bZSi8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# First chart gives the optimal number of clusters. We get this chart by validation and basis of some rules"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i have use Elbow Method for optimal number of k.\n",
        "\n",
        "The elbow method is a graphical representation of finding the optimal 'K' in a K-means clustering. It works by finding WCSS (Within-Cluster Sum of Square). i.e. the sum of the square distance between points in a cluster and the cluster centroid.\n",
        "\n",
        "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can not directly predict the number of cluster. After using Elbow method we can get optimal number of clusters and we can implement it directly."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  Hierarchical Clustering (Agglomerative Clustering)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the dendrogram to decide on the optimal number of clusters for the agglomerative (hierarchical) clustering algorithm:\n",
        "\n"
      ],
      "metadata": {
        "id": "mOdZMlmqS9VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a dendogram to decide on the number of clusters. it takes time.\n",
        "plt.figure(figsize=(10, 7))\n",
        "dend = shc.dendrogram(shc.linkage(x_pca, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Netflix Shows')\n",
        "plt.ylabel('Distance')\n",
        "plt.axhline(y= 3.8, color='r', linestyle='--')"
      ],
      "metadata": {
        "id": "RP7MslvbS_ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At a distance of 3.8 units, 9 clusters can be built using the agglomerative clustering algorithm.\n",
        "\n",
        "Building 9 clusters using the Agglomerative clustering algorithm:"
      ],
      "metadata": {
        "id": "HyqhJP7xTChE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering model\n",
        "hierarchical = AgglomerativeClustering(n_clusters=9, affinity='euclidean', linkage='ward')\n",
        "hierarchical.fit_predict(x_pca)"
      ],
      "metadata": {
        "id": "aU2kz2lFTDMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a kmeans cluster number attribute\n",
        "df['hierarchical_cluster'] = hierarchical.labels_\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cSA0qEpiTHl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of movies and tv shows in each cluster\n",
        "plt.figure(figsize=(10,5))\n",
        "q = sns.countplot(x='hierarchical_cluster',data=df, hue='type')\n",
        "plt.title('Number of movies and tv shows in each cluster - Hierarchical Clustering')\n",
        "for i in q.patches:\n",
        "  q.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n"
      ],
      "metadata": {
        "id": "cCmXO8ZqTLQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successfully built 12 clusters using the Agglomerative (hierarchical) clustering algorithm.\n",
        "\n",
        "Building wordclouds for different clusters built."
      ],
      "metadata": {
        "id": "iBd8RFU3TNxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a wordcloud for the movie descriptions\n",
        "def hierarchical_worldcloud(cluster_num):\n",
        "  comment_words = ''\n",
        "  stopwords = set(STOPWORDS)\n",
        "\n",
        "  # iterate through the csv file\n",
        "  for val in df[df['hierarchical_cluster']==cluster_num].description.values:\n",
        "\n",
        "      # typecaste each val to string\n",
        "      val = str(val)\n",
        "\n",
        "      # split the value\n",
        "      tokens = val.split()\n",
        "\n",
        "      # Converts each token into lowercase\n",
        "      for i in range(len(tokens)):\n",
        "          tokens[i] = tokens[i].lower()\n",
        "\n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "  wordcloud = WordCloud(width = 700, height = 700,\n",
        "                  background_color ='white',\n",
        "                  stopwords = stopwords,\n",
        "                  min_font_size = 10).generate(comment_words)\n",
        "\n",
        "\n",
        "  # plot the WordCloud image\n",
        "  plt.figure(figsize = (10,5), facecolor = None)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)"
      ],
      "metadata": {
        "id": "J1X7YutmTSUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 0\n",
        "hierarchical_worldcloud(0)"
      ],
      "metadata": {
        "id": "IqmsaUTHTVUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 0: life, new, find, family, world, friend, young, two, take\n",
        "\n"
      ],
      "metadata": {
        "id": "ptIgZdo1TZA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 1\n",
        "hierarchical_worldcloud(1)"
      ],
      "metadata": {
        "id": "uyYBiDXoTa0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 1: life, find, love, family, friend, new, young, home, woman, daughter, father\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXUNXVLvTdqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 2\n",
        "hierarchical_worldcloud(2)"
      ],
      "metadata": {
        "id": "p5UDPVnqTePi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 2: love, life, family, find, woman, student, school, friend\n",
        "\n"
      ],
      "metadata": {
        "id": "hyN_OEr9TiTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 3\n",
        "hierarchical_worldcloud(3)"
      ],
      "metadata": {
        "id": "YpOpRMNJTi9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 3: documentry, world, family, life, live, explore, team, woman, murder, young, history, series\n",
        "\n"
      ],
      "metadata": {
        "id": "eaGm8P3ITmDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 4\n",
        "hierarchical_worldcloud(4)"
      ],
      "metadata": {
        "id": "D5sIvv6ETmxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 4: stand, special, comedian, stage, show, star, comic, comedy, life, culture\n",
        "\n"
      ],
      "metadata": {
        "id": "p929qYkuTqP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 5\n",
        "hierarchical_worldcloud(5)"
      ],
      "metadata": {
        "id": "ZjRWoxYyTq_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 5: love, family, life, man, young, father, woman, find, fall, india, mumbai, girl\n",
        "\n"
      ],
      "metadata": {
        "id": "OUCm6HFyTuo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 6\n",
        "hierarchical_worldcloud(6)"
      ],
      "metadata": {
        "id": "O9bokCstTwfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 6: young, world, girl, battle, friend, year, demon, student, school, father, group, mysterious\n",
        "\n"
      ],
      "metadata": {
        "id": "ey5S9wsNTzXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 7\n",
        "hierarchical_worldcloud(7)"
      ],
      "metadata": {
        "id": "uzp8eEIKT1KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 7: love, life, woman, new, find, family, korea, secret, world, live, young, two, student\n",
        "\n"
      ],
      "metadata": {
        "id": "UoSG6X6ZT3aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 8\n",
        "hierarchical_worldcloud(8)"
      ],
      "metadata": {
        "id": "ozHK1IhaT4EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 8: woman, man, life, find, egyptian, wealthy, father, young, love, criminal, struggling\n",
        "\n"
      ],
      "metadata": {
        "id": "lMea2XnsT7iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# In wordcloud we observe most popular keywords in different clusters\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i used Agglomerative Clustering.\n",
        "\n",
        "Agglomerative Clustering is a type of hierarchical clustering algorithm. It is an unsupervised machine learning technique that divides the population into several clusters such that data points in the same cluster are more similar and data points in different clusters are dissimilar.\n",
        "\n",
        "Agglomerative Hierarchical Clustering (AHC) is an iterative classification method whose principle is simple.\n",
        "\n",
        "(1) The process starts by calculating the dissimilarity between the N objects.\n",
        "\n",
        "(2) Then two objects which when clustered together minimize a given agglomeration criterion, are clustered together thus creating a class comprising these two objects.\n",
        "\n",
        "(3) Then the dissimilarity between this class and the N-2 other objects is calculated using the agglomeration criterion. The two objects or classes of objects whose clustering together minimizes the agglomeration criterion are then clustered together.\n",
        "\n",
        "This process continues until all the objects have been clustered."
      ],
      "metadata": {
        "id": "lN2tU-IWUA-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Here for optimal number of clusters i have use Dendrogram. We decided the number of clusters basis on the some rules and analysis of the graph\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have use Dendrogram for optimal number of clusters.\n",
        "\n",
        "A dendrogram is a branching diagram that represents the relationships of similarity among a group of entities. Each branch is called a clade. There is no limit to the number of leaves in a clade.\n",
        "\n",
        "A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes. To avoid crossing lines, the diagram is graphically arranged so that members of each pair of classes to be merged are neighbors in the diagram. The Dendrogram tool uses a hierarchical clustering algorithm.\n",
        "\n",
        "A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation. The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending on the selected distance measure."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get optimal number of cluster is 9.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we cannot directly predict the number of cluster . After plotting dendrogram chart we can get optimal number of clusters and we can implement it directly in the data."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Finding optimal number of clusters using the Silhouette Score. it takes time.\n",
        "for n_clusters in range(2,15):\n",
        "  km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n",
        "  km.fit(x_pca)\n",
        "  preds = km.predict(x_pca)\n",
        "  centers = km.cluster_centers_\n",
        "  score = silhouette_score(x_pca, preds, metric='euclidean')\n",
        "  print (\"For n_clusters = %d, silhouette score is %0.4f\"%(n_clusters, score))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this chart we can say that optimal number of cluster is 13. Because the silhouette score is highest for the cluster 13.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xd387-4XUSoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Silhouette Plots for Each Clusters\n",
        "\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "for n_clusters in range(2,15):\n",
        "    km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n",
        "    km.fit(x_pca)\n",
        "    preds = km.predict(x_pca)\n",
        "    centers = km.cluster_centers_\n",
        "\n",
        "    # Set parameters and labels\n",
        "    score = silhouette_score(x_pca, preds, metric='euclidean')\n",
        "    print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "\n",
        "    visualizer = SilhouetteVisualizer(km)\n",
        "\n",
        "    visualizer.fit(x_pca) # Fit the training data to the visualizer\n",
        "    visualizer.poof() # Draw/show/poof the data"
      ],
      "metadata": {
        "id": "hPHe91FgUU8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Here we define the number of clusters basis on the Silhouette Cofficient\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of cluster is defined by Silhouette Coefficient.\n",
        "\n",
        "Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n",
        "\n",
        "1: Means clusters are well apart from each other and clearly distinguished.\n",
        "\n",
        "0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "\n",
        "-1: Means clusters are assigned in the wrong way.\n",
        "\n",
        "The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of."
      ],
      "metadata": {
        "id": "8FZH7GpQUeN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# We decided the number of clusters basis on the some rules and analysis of the graph\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we got Silhouette Coefficient for optimal number of clusters. From this data we got optimal number of clusters is 13 because it has a higher Silhouette Coefficient."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we cannot directly predict the number of cluster . After using this method we can get optimal number of clusters and we can implement it directly in data."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score is the best evaluation metric for optimization the number of clusters.\n",
        "\n",
        "The optimal number of cluster gives us the lightness and transparency of the business.\n",
        "\n",
        "Through cluster we can find out which type of customers are in our data.\n",
        "\n",
        "This evaluation metric makes business decision easier. Getting the Silhouette score is very easy."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the charts we can see that K-Mean Clustering model is best model for our data.\n",
        "\n",
        "Here we get optimal number of clusters is 6, but often the number of clusters is already determined within the business. If the number of clusters within a business is already determined, we can apply the algorithm well.\n",
        "\n",
        "Within the K-Mean Cluster graph we can see that the clusters are well divided.\n",
        "\n",
        "Through this cluster we can know what type of data is in which cluster.\n",
        "\n",
        "The goal of this problems may be to discover groups of similar examples within the data.\n",
        "\n",
        "The primary function of this algorithm is to perform segmentation, whether it is store, product, or customer. Customers and products can be clustered into hierarchical groups based on different attributes."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model explanation of Hierarchical Clustering\n",
        "\n",
        "1.  Import the necessary libraries\n",
        "2.  Load the data\n",
        "3.  Preprocess the data\n",
        "4.  Perform PCA to reduce the dimensionality of the data\n",
        "5.  Create the Agglomerative Clustering model\n",
        "6.  Fit the model to the data\n",
        "7.  Get the cluster labels\n",
        "8.  Calculate the Silhouette Coefficient\n",
        "9.  Print the Silhouette Coefficient\n",
        "10. Visualize the clusters"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.** **Content Based Recommender System**"
      ],
      "metadata": {
        "id": "Gpn6DXaqVJ9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a new dataframe for building a recommender system\n",
        "\n",
        "recommender_df = df1.copy()"
      ],
      "metadata": {
        "id": "opPiIWa3VWdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the index of the df from show id to show title\n",
        "\n",
        "recommender_df['show_id'] = recommender_df.index"
      ],
      "metadata": {
        "id": "fEP9PmxdVcad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting tokens to string\n",
        "\n",
        "def convert(lst):\n",
        "  return ' '.join(map(str, lst))\n",
        "\n",
        "recommender_df['clustering'] = recommender_df['clustering'].apply(lambda x: convert(x))"
      ],
      "metadata": {
        "id": "RNUnDkFuVedn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting title of movies/Tv shows as index\n",
        "\n",
        "recommender_df.set_index('title',inplace=True)"
      ],
      "metadata": {
        "id": "3kXEdYJUVhFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count vectorizer\n",
        "\n",
        "CV = CountVectorizer()\n",
        "converted_matrix = CV.fit_transform(recommender_df['clustering'])"
      ],
      "metadata": {
        "id": "gT-ItIyKVjcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity\n",
        "\n",
        "cosine_similarity = cosine_similarity(converted_matrix)"
      ],
      "metadata": {
        "id": "K-iU47pkVlQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity.shape"
      ],
      "metadata": {
        "id": "hxh2a7BhVm6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Developing a function to get 10 recommendations for a show\n",
        "\n",
        "indices = pd.Series(recommender_df.index)\n",
        "\n",
        "def recommend_10(title, cosine_sim = cosine_similarity):\n",
        "  try:\n",
        "    recommend_content = []\n",
        "    idx = indices[indices == title].index[0]\n",
        "    series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
        "    top10 = list(series.iloc[1:11].index)\n",
        "    # list with the titles of the best 10 matching movies\n",
        "    for i in top10:\n",
        "      recommend_content.append(list(recommender_df.index)[i])\n",
        "    print(\"If you liked '\"+title+\"', you may also enjoy:\\n\")\n",
        "    return recommend_content\n",
        "\n",
        "  except:\n",
        "    return 'Invalid Entry'"
      ],
      "metadata": {
        "id": "rtKpA6YmVpZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendations for 'A Man Called God'\n",
        "recommend_10('A Man Called God')"
      ],
      "metadata": {
        "id": "JmtesW22VrPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendations for 'Stranger Things'\n",
        "recommend_10('Stranger Things')"
      ],
      "metadata": {
        "id": "FrB5CvDqVtEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendations for 'Peaky Blinders'\n",
        "recommend_10('Peaky Blinders')"
      ],
      "metadata": {
        "id": "gCU41Xy3Vu7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendations for 'Lucifer'\n",
        "recommend_10('Lucifer')"
      ],
      "metadata": {
        "id": "_Sk_U8KmVw2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a Indian Movie\n",
        "recommend_10('Zindagi Na Milegi Dobara')"
      ],
      "metadata": {
        "id": "jX63gHK-VzJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a International Movie\n",
        "recommend_10('Avengers: Infinity War')"
      ],
      "metadata": {
        "id": "bKmurxKZV1F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a Korean TV Show\n",
        "recommend_10('What in the World Happened?')"
      ],
      "metadata": {
        "id": "APGOxRv3V3Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Recommender System on a Content, Which is Not Listed in Netflix Dataset\n",
        "recommend_10('Avenger')"
      ],
      "metadata": {
        "id": "gaQutsGOV5Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***9.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Serialize process (wb=write byte)\n",
        "# Save the best model (KMeans Clustering)\n",
        "pickle.dump(kmean,open('kmeans_model.pkl','wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# Unserialize process (rb=read byte)\n",
        "pickled_model= pickle.load(open('kmeans_model.pkl','rb'))\n",
        "\n",
        "# Predicting the unseen data\n",
        "pickled_model.predict(x_pca)"
      ],
      "metadata": {
        "id": "whhh8CXhV_7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_kmean"
      ],
      "metadata": {
        "id": "qEnctL3WWCsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of the project was to cluster TV shows and movies based on their similarities and differences, with the ultimate goal of creating a content-based recommender system that recommends 10 shows to users based on their viewing history. Some key points from the project include:\n",
        "\n",
        "*  Exploring the dataset consist of 7787 records and 12 attributes, with a focus on missing value imputation and exploratory data analysis (EDA).\n",
        "\n",
        "*  The analysis revealed that Netflix has a greater number of movies than TV shows, with a rapidly growing collection of shows from the United States.\n",
        "\n",
        "*  To cluster the shows, i have selected six key attributes: director, cast, country, genre, rating, and description (all are categorical variables). These attributes were transformed into a 20000-feature TF-IDF vectorization, and Principal Component Analysis (PCA) was used to address the curse of dimensionality. Captured more than 80% of the variance by reducing the components to 3400.\n",
        "\n",
        "*  Next, i used K-Means and Agglomerative clustering algorithms to group the shows. The elbow method confirmed that the optimal number of clusters was 6 for K-Means, however for Silhouette score analysis it was 13.\n",
        "\n",
        "*  In Agglomerative clustering the optimal number of clusters was 9, which we visualized with a dendrogram.\n",
        "\n",
        "*  Continued all the efforts by creating a content-based recommender system using the similarity matrix obtained through cosine similarity.\n",
        "\n",
        "The recommender system offers personalized recommendations based on the type of shows the user has watched and provides the user with ten top-notch suggestions to explore."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}